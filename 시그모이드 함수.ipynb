{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시그모이드 함수(Sigmoid Function)\n",
    "\n",
    "#### [참고블로그: http://taewan.kim/post/sigmoid_diff/]('http://taewan.kim/post/sigmoid_diff/')\n",
    "\n",
    "\n",
    "* x의 계수가 바뀔수 있는데 (e^(-ax)), 이 때 a를 게인(gain)이라고 함\n",
    "* 아래 그림처럼 a = 1 일 때, 표준 시그모이드함수라고 함. 별 다른 언급 없이 그냥 시그모이드 함수라고도 함\n",
    "* x가 음의 무한대로 갈수록 분모는 양의 무한대가 되어 함수값은 0으로 수렴하고, 반대의 경우 1로 수렴한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://taewanmerepo.github.io/2017/09/sigmoid/post.jpg' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모든 입력값에 대해 시그모이드는 미분 가능한 0과 1사이의 값을 반환하기 때문에, Logistic Classification과 같은 분류문제의 가설과 비용함수(Cost function)에 많이 사용\n",
    "\n",
    "* 반환 값이 확률형태이기 때문에 결과를 확률로 해석할 때도 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------이하로는 딥러닝을 더 공부해야 알 부분---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 딥러닝에서는 노드에 임계값을 넘을 때만 출력하는 활성함수로 이용.시그모이드 함수는 마이너스 값을 0에 가깝게 표현하기 때문에 입력값이 최종계층에서 미치는 영향이 적어지는 Vanishing gradient problem이 발생 --> 이 때문에 실무에서는 잘 쓰지 않고, 딥러닝 입문과정에서 다룸\n",
    " \n",
    "   - 활성함수 : 어떤 신호를 입력받아 적절한 처리를 하여 출력을 해주는 함수로 이를 통해 출력된 신호가 다음 단계에서 활성화되는지 결정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid 함수의 미분\n",
    "\n",
    "시그모이드를 logistic classification의 가설로 사용하거나 딥러닝의 활성함수로 사용할 경우 경사하강법(Gradient Descent Algorithm) 계산 혹은 역전파 계산 과정에서 시그모이드의 미분이 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F9957644A5A6031512503AF' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 미분 결과는 위와 같이 매우 간결한 결과로 나타나고, 이를 그래프로 그리면 아래와 같음\n",
    "\n",
    "  - 미분계수를 보면 최댓값은 0.25\n",
    "  - deep learning에서 학습을 위하여 역전파(Backpropagation)를 계산하는 과정에서 activation function의 미분 값을 곱하는 과정이 포함. sigmoid를 활성 함수로 사용하는 경우 은닉층의 깊이가 깊다면 오차율 계산이 어렵다는 문제가 발생합니다. 이것이 딥러닝에 sigmoid를 사용할 때 “vanishing gradient problem“이 발행하는 이유."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='https://taewanmerepo.github.io/2017/09/sigmoid/differential_sigmoid.jpg' width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이썬으로 구현한 시그모이드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x, deff=False):\n",
    "    if deff:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    else :\n",
    "        return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19661193324148185"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
